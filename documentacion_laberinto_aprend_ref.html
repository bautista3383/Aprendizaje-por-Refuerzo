<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Laberinto con Aprendizaje por Refuerzo</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0a0a28; /* Similar al fondo del juego */
            color: #e0e0ff;
        }
        .card {
            background-color: #141440;
            border: 1px solid #205096;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(32, 80, 150, 0.5);
        }
        .accordion-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease-out;
        }
        .accordion-button.active + .accordion-content {
            max-height: 1000px; /* Suficientemente grande para el contenido */
        }
        .code-snippet {
            background-color: #050518;
            border-left: 4px solid #facc15;
            color: #a5b4fc;
            white-space: pre-wrap;
            font-size: 0.875rem;
        }
        .json-sim {
            background-color: #050518;
            border: 1px solid #205096;
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: monospace;
            font-size: 0.9rem;
            white-space: pre;
            overflow-x: auto;
        }
        .json-sim .key { color: #93c5fd; }
        .json-sim .string { color: #a5b4fc; }
        .json-sim .number { color: #facc15; }
        .json-sim .comment { color: #6b7280; animation: fadeIn 1s ease-in-out; }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
    </style>
</head>
<body class="antialiased">

    <div class="container mx-auto p-4 md:p-8">
        
        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-4">An√°lisis de un Agente Aut√≥nomo en Laberintos mediante Aprendizaje por Refuerzo</h1>
            <p class="text-lg text-indigo-300 max-w-3xl mx-auto">Documentaci√≥n t√©cnica sobre la implementaci√≥n de un agente inteligente para la resoluci√≥n de problemas de navegaci√≥n.</p>
        </header>

        <!-- Secci√≥n de Conceptos -->
        <section id="conceptos" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8 text-yellow-300">Fundamentos del Aprendizaje por Refuerzo (RL)</h2>
            <div class="grid md:grid-cols-2 gap-8 items-center">
                <div>
                    <p class="mb-4 text-lg">
                        El Aprendizaje por Refuerzo es un √°rea del Machine Learning donde un agente computacional aprende a tomar decisiones a trav√©s de la interacci√≥n directa con un entorno. A diferencia de otros paradigmas, no se le proveen datos etiquetados.
                    </p>
                    <p class="text-lg">
                        El agente ejecuta acciones y recibe retroalimentaci√≥n en forma de recompensas o penalizaciones. El objetivo del agente es desarrollar una pol√≠tica (una estrategia de toma de decisiones) que maximice la recompensa acumulada a lo largo del tiempo.
                    </p>
                </div>
                <div class="flex justify-center items-center p-4 card rounded-lg">
                    <!-- Diagrama T√©cnico del Ciclo de RL -->
                    <svg viewBox="0 0 400 220" class="w-full max-w-md">
                        <defs>
                            <marker id="arrow" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse">
                                <path d="M 0 0 L 10 5 L 0 10 z" fill="#e0e0ff" />
                            </marker>
                        </defs>
                        <!-- Agente -->
                        <rect x="20" y="80" width="100" height="60" rx="10" fill="#205096" stroke="#a5b4fc" stroke-width="2"/>
                        <text x="70" y="115" font-size="18" fill="white" text-anchor="middle">Agente</text>
                        <!-- Entorno -->
                        <rect x="280" y="80" width="100" height="60" rx="10" fill="#205096" stroke="#a5b4fc" stroke-width="2"/>
                        <text x="330" y="115" font-size="18" fill="white" text-anchor="middle">Entorno</text>
                        <!-- Flecha Acci√≥n -->
                        <path d="M 125 100 Q 200 60 275 90" stroke="#facc15" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                        <text x="200" y="65" font-size="14" fill="#facc15" text-anchor="middle">Acci√≥n (A_t)</text>
                        <!-- Flecha Estado/Recompensa -->
                        <path d="M 275 130 Q 200 180 125 120" stroke="#a5b4fc" stroke-width="2" fill="none" marker-end="url(#arrow)"/>
                        <text x="200" y="175" font-size="14" fill="#a5b4fc" text-anchor="middle">Estado (S_t+1) + Recompensa (R_t+1)</text>
                    </svg>
                </div>
            </div>
            
            <h3 class="text-2xl font-bold text-center mt-12 mb-6 text-yellow-300">Componentes Fundamentales del Modelo</h3>
            <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div class="card p-6 rounded-lg text-center">
                    <span class="text-5xl mb-4 block">ü§ñ</span>
                    <h4 class="text-xl font-semibold mb-2 text-white">Agente</h4>
                    <p>La entidad computacional que toma decisiones. En esta implementaci√≥n, es la l√≥gica que controla a los avatares üê≠, üê∞, o üêù.</p>
                </div>
                <div class="card p-6 rounded-lg text-center">
                    <span class="text-5xl mb-4 block">üó∫Ô∏è</span>
                    <h4 class="text-xl font-semibold mb-2 text-white">Entorno</h4>
                    <p>El sistema con el que el agente interact√∫a. Aqu√≠, es la estructura del laberinto, que define los estados posibles y las transiciones entre ellos.</p>
                </div>
                <div class="card p-6 rounded-lg text-center">
                    <span class="text-5xl mb-4 block">üïπÔ∏è</span>
                    <h4 class="text-xl font-semibold mb-2 text-white">Acci√≥n</h4>
                    <p>El conjunto de operaciones que el agente puede ejecutar. En este caso, un conjunto discreto de cuatro acciones: mover arriba, abajo, izquierda o derecha.</p>
                </div>
                <div class="card p-6 rounded-lg text-center">
                    <span class="text-5xl mb-4 block">üèÜ</span>
                    <h4 class="text-xl font-semibold mb-2 text-white">Recompensa</h4>
                    <p>Una se√±al escalar que el entorno env√≠a al agente como evaluaci√≥n de su √∫ltima acci√≥n. Es la base para la optimizaci√≥n de la pol√≠tica del agente.</p>
                </div>
            </div>
        </section>

        <!-- Secci√≥n de Par√°metros -->
        <section id="parametros" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8 text-yellow-300">Configuraci√≥n de Par√°metros del Modelo</h2>
            <div class="space-y-4">
                <!-- Acorde√≥n de Recompensas -->
                <div class="card rounded-lg">
                    <button class="accordion-button w-full text-left p-4 font-semibold text-white text-lg flex justify-between items-center">
                        <span>üéØ Funci√≥n de Recompensa</span>
                        <span class="transform transition-transform duration-300">+</span>
                    </button>
                    <div class="accordion-content px-4 pb-4">
                        <p class="mb-2">La funci√≥n de recompensa define el objetivo del problema. Su dise√±o es cr√≠tico para guiar al agente hacia el comportamiento deseado.</p>
                        <ul class="list-disc list-inside space-y-2">
                            <li><strong class="text-green-400">GOAL_REWARD = 100:</strong> Una recompensa positiva alta al alcanzar el estado objetivo (la salida del laberinto).</li>
                            <li><strong class="text-red-400">WALL_PENALTY = -20:</strong> Una penalizaci√≥n negativa significativa por realizar una acci√≥n que resulta en una colisi√≥n con un muro.</li>
                            <li><strong class="text-yellow-400">MOVE_REWARD = -0.01:</strong> Una peque√±a penalizaci√≥n por cada paso. Esto introduce un costo temporal, incentivando al agente a encontrar la ruta m√°s corta.</li>
                        </ul>
                    </div>
                </div>

                <!-- Acorde√≥n de Q-Learning -->
                <div class="card rounded-lg">
                    <button class="accordion-button w-full text-left p-4 font-semibold text-white text-lg flex justify-between items-center">
                        <span>üß† Algoritmo Q-Learning</span>
                        <span class="transform transition-transform duration-300">+</span>
                    </button>
                    <div class="accordion-content px-4 pb-4">
                        <p class="mb-2">Se implementa el algoritmo Q-Learning, un m√©todo de RL sin modelo. Este busca aprender una funci√≥n de valor acci√≥n-estado (Q-value) que estima la recompensa futura esperada al tomar una acci√≥n 'a' en un estado 's'.</p>
                        <ul class="list-disc list-inside space-y-2">
                            <li><strong class="text-cyan-400">LEARNING_RATE = 0.1 (Tasa de Aprendizaje Œ±):</strong> Determina en qu√© medida la nueva informaci√≥n anula la informaci√≥n antigua. Un valor de 0.1 indica un aprendizaje gradual y estable.</li>
                            <li><strong class="text-purple-400">DISCOUNT_FACTOR = 0.95 (Factor de Descuento Œ≥):</strong> Pondera la importancia de las recompensas futuras. Un valor cercano a 1 (0.95) indica que el agente tiene una visi√≥n a largo plazo.</li>
                        </ul>
                    </div>
                </div>
                
                <!-- Acorde√≥n de Exploraci√≥n -->
                <div class="card rounded-lg">
                    <button class="accordion-button w-full text-left p-4 font-semibold text-white text-lg flex justify-between items-center">
                        <span>üß≠ Pol√≠tica de Exploraci√≥n (Epsilon-Greedy)</span>
                        <span class="transform transition-transform duration-300">+</span>
                    </button>
                    <div class="accordion-content px-4 pb-4">
                        <p class="mb-2">Para asegurar que el agente explore suficientemente el espacio de estados, se utiliza una pol√≠tica Œµ-greedy. Esta equilibra la explotaci√≥n del conocimiento actual con la exploraci√≥n de nuevas acciones.</p>
                        <ul class="list-disc list-inside space-y-2">
                            <li><strong class="text-pink-400">EPSILON_START = 1.0:</strong> El valor inicial de Œµ es 1.0, lo que significa que al principio, el 100% de las acciones son aleatorias (exploraci√≥n pura).</li>
                            <li><strong class="text-orange-400">EPSILON_DECAY = 0.995:</strong> Despu√©s de cada episodio, Œµ se multiplica por este factor, reduciendo gradualmente la probabilidad de exploraci√≥n.</li>
                            <li><strong class="text-teal-400">EPSILON_END = 0.01:</strong> El valor m√≠nimo de Œµ es 0.01, asegurando que siempre haya una m√≠nima probabilidad (1%) de exploraci√≥n, incluso al final del entrenamiento.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Secci√≥n de L√≥gica del Juego -->
        <section id="logica" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8 text-yellow-300">L√≥gica de Ejecuci√≥n y Simulaci√≥n</h2>
            <div class="space-y-6">
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Ciclo de Entrenamiento (Episodios)</h3>
                    <p>El entrenamiento se realiza a lo largo de **1000 episodios (EPISODES = 1000)**. Un episodio es un intento completo, desde el estado inicial hasta un estado terminal (meta o l√≠mite de pasos). Este n√∫mero elevado de iteraciones es necesario para que la Q-Table converja a valores estables. Se visualizan los episodios **1, 50 y 200** como puntos de control para observar la evoluci√≥n de la pol√≠tica del agente.</p>
                </div>
                 <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">L√≠mite de Pasos por Episodio</h3>
                    <p>Para prevenir bucles infinitos durante la fase de exploraci√≥n, cada episodio est√° limitado por un n√∫mero m√°ximo de pasos. Este se calcula din√°micamente con la f√≥rmula:</p>
                    <p class="code-snippet p-3 my-2 rounded-md font-mono text-sm">max_steps = width * height * 2</p>
                    <p>Para los laberintos de 19x9, el l√≠mite es de **342 pasos**. Si el agente alcanza este l√≠mite, el episodio termina y se considera no resuelto.</p>
                </div>
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Selecci√≥n de Entornos (Laberintos)</h3>
                    <p>El sistema implementa una l√≥gica de selecci√≥n de laberintos para garantizar la exposici√≥n a todos los entornos al inicio:</p>
                    <ul class="list-decimal list-inside mt-2">
                        <li><strong>Primeras 3 Partidas:</strong> Se utiliza una cola pre-mezclada de los tres laberintos disponibles, asegurando que cada uno se juegue una vez sin repetici√≥n.</li>
                        <li><strong>Partidas Subsecuentes:</strong> La selecci√≥n del laberinto se realiza de forma uniformemente aleatoria entre los tres disponibles.</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <!-- Secci√≥n de C√≥digo -->
        <section id="codigo" class="mb-16">
            <h2 class="text-3xl font-bold text-center mb-8 text-yellow-300">Estructura del C√≥digo Fuente</h2>
            <div class="space-y-6">
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Librer√≠as Utilizadas</h3>
                    <ul class="list-disc list-inside mt-2 space-y-1">
                        <li><strong>Pygame:</strong> Utilizada para la creaci√≥n de la interfaz gr√°fica, la renderizaci√≥n del laberinto, el manejo de eventos de teclado y el control del tiempo.</li>
                        <li><strong>NumPy:</strong> Fundamental para la gesti√≥n eficiente de la Q-Table, que es una matriz num√©rica. Permite realizar operaciones matem√°ticas vectorizadas.</li>
                        <li><strong>JSON:</strong> Se emplea para la serializaci√≥n y deserializaci√≥n de los datos de estad√≠sticas y logs, permitiendo guardarlos y cargarlos de forma estructurada.</li>
                        <li><strong>OS, sys, time, datetime, random, subprocess:</strong> Librer√≠as est√°ndar de Python para la interacci√≥n con el sistema operativo, manejo de tiempo, generaci√≥n de n√∫meros aleatorios y ejecuci√≥n de procesos externos.</li>
                    </ul>
                </div>
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Clase `Maze`</h3>
                    <p>Abstrae el entorno del laberinto. Gestiona la representaci√≥n del estado, las transiciones de estado y la funci√≥n de recompensa. Su m√©todo principal, `step()`, procesa una acci√≥n del agente y devuelve el nuevo estado, la recompensa obtenida y una bandera booleana que indica si el estado es terminal.</p>
                    <pre class="code-snippet p-3 mt-2 rounded-md font-mono">def step(self, state, action):
    current_pos = self.get_pos_for_state(state)
    new_pos = self._get_new_pos_from_action(current_pos, action)

    if not (...condiciones de borde y pared...):
        return state, WALL_PENALTY, False, "Collision"

    if new_pos == self.goal_pos:
        return self.get_state_for_pos(new_pos), GOAL_REWARD, True, "Goal"
    
    return self.get_state_for_pos(new_pos), MOVE_REWARD, False, "Move"</pre>
                </div>
                 <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Clase `Agent`</h3>
                    <p class="mb-2">Implementa la l√≥gica del agente de RL. Su componente central es la **Q-Table**, una matriz que almacena el valor esperado de cada acci√≥n en cada estado del laberinto. Los m√©todos principales son:</p>
                    <ul class="list-disc list-inside space-y-2">
                        <li><strong>choose_action():</strong> Implementa la pol√≠tica **Œµ-greedy**. Con una probabilidad Œµ, elige una acci√≥n al azar (exploraci√≥n). Con probabilidad 1-Œµ, elige la mejor acci√≥n conocida seg√∫n la Q-Table (explotaci√≥n).</li>
                        <li><strong>learn():</strong> Aplica la regla de actualizaci√≥n de Q-Learning. Despu√©s de cada acci√≥n, ajusta el valor en la Q-Table para reflejar la recompensa obtenida y el valor m√°ximo del nuevo estado, "refinando" as√≠ el conocimiento del agente.</li>
                    </ul>
                     <pre class="code-snippet p-3 mt-2 rounded-md font-mono">def learn(self, state, action, reward, next_state):
    old_value = self.q_table[state, action]
    next_max = np.max(self.q_table[next_state, :])
    new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max - old_value)
    self.q_table[state, action] = new_value</pre>
                </div>
                 <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Clase `Game`</h3>
                    <p>Act√∫a como el controlador principal de la simulaci√≥n. Gestiona la inicializaci√≥n de Pygame, el bucle principal del men√∫, la orquestaci√≥n del ciclo de entrenamiento de 1000 episodios y la ejecuci√≥n de la pol√≠tica aprendida. Tambi√©n invoca los m√≥dulos de registro de datos.</p>
                    <pre class="code-snippet p-3 mt-2 rounded-md font-mono">def run_game(self, maze_info, maze_name):
    ...
    agent = Agent(...)
    for episode in range(1, EPISODES + 1):
        # Bucle de entrenamiento silencioso
        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _ = maze.step(state, action)
            agent.learn(state, action, reward, next_state)
            ...
        agent.decay_epsilon()
    ...
    # Ejecuci√≥n final con la pol√≠tica aprendida
    final_run_data = self.run_single_episode(..., deterministic=True, ...)</pre>
                </div>
            </div>
        </section>

        <!-- Secci√≥n de Salida -->
        <section id="salida">
            <h2 class="text-3xl font-bold text-center mb-8 text-yellow-300">Generaci√≥n de Resultados y Estad√≠sticas</h2>
            <div class="grid md:grid-cols-2 gap-8">
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Estructura del Archivo JSON</h3>
                    <p>Al finalizar cada partida, se genera o actualiza un archivo `.json` que sirve como base de datos. A continuaci√≥n se muestra una simulaci√≥n de su estructura:</p>
                    <div id="json-sim-container" class="json-sim mt-2"></div>
                </div>
                <div class="card p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2 text-white">Generaci√≥n de la Imagen de Salida</h3>
                    <p>Tras la finalizaci√≥n del entrenamiento, se ejecuta una simulaci√≥n final utilizando la pol√≠tica √≥ptima aprendida (explotaci√≥n pura). Al concluir esta simulaci√≥n, se realiza una captura de la ventana de Pygame.</p>
                    <p class="mt-2">Esta imagen, guardada en formato `.png`, sirve como evidencia visual del rendimiento del agente, mostrando la trayectoria final que este determin√≥ como la m√°s eficiente.</p>
                </div>
            </div>
        </section>

    </div>

    <script>
        document.querySelectorAll('.accordion-button').forEach(button => {
            button.addEventListener('click', () => {
                const content = button.nextElementSibling;
                button.classList.toggle('active');
                const icon = button.querySelector('span:last-child');
                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                    icon.style.transform = 'rotate(0deg)';
                } else {
                    content.style.maxHeight = content.scrollHeight + "px";
                    icon.style.transform = 'rotate(45deg)';
                } 
            });
        });

        // Simulaci√≥n de JSON
        const jsonContainer = document.getElementById('json-sim-container');
        const lines = [
            `{`,
            `  <span class="key">"estadisticas_por_laberinto"</span>: { ... },`,
            `  <span class="key">"historial_partidas"</span>: [`,
            `    {`,
            `      <span class="key">"partida_numero"</span>: <span class="number">1</span>,`,
            `      <span class="key">"laberinto"</span>: <span class="string">"Ayuda_al_raton..."</span>,`,
            `      <span class="key">"resumen_resultado"</span>: { ... },`,
            `      <span class="key">"detalle_aprendizaje"</span>: {`,
            `        <span class="key">"intento_1"</span>: { <span class="comment">// Log del primer intento...</span> },`,
            `        <span class="key">"intento_50"</span>: { <span class="comment">// Log del intento 50...</span> }`,
            `      }`,
            `    }`,
            `  ]`,
            `}`
        ];
        
        let lineIndex = 0;
        function typeLine() {
            if (lineIndex < lines.length) {
                jsonContainer.innerHTML += lines[lineIndex] + '\n';
                lineIndex++;
                setTimeout(typeLine, 300);
            }
        }
        typeLine();
    </script>

</body>
</html>
